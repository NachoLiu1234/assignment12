{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是encoder-decoder 结构, 这样的结构可以用神经网络对sequence或者图片进行压缩, 获得一个缩小的数据, 在通过decoder进行神经网络的解压, 获得和之前差不多或者完全一样的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greddy search 是对seq2seq的decoder的结果只返回概率最大的一个, beam search是返回几个概率最大的, 知道碰到eos是概率最大的一个, 最后计算总体概率最大的一个, 因为greddy search计算出的概率最大的一个并不一定是总的最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在训练时, 把a(1)...a(n)的加权作为content, 权重是e, e=S(t-1) * a(t)或者S(t-1)*W* a(t)或者V.T*tanh(W1*a(1) + W2 * S(t-1)), 在预测时, 通过上一个的预测值作为下一个的输出值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embeding只能表示一个词的固定的词向量, 但是有的词是一词多义的, 需要用elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一层或多层双向的lstm, 输入是词的word2vector, 把每一层的输出和输入层做一个加权平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN在很多次传递h后, 前面的就影响很小了, Transformer使用了Self-Attention, 用q*k计算出x1在所有的x的位置的softmax, 再乘以每个v并加和获得x1处的z值, Wq和Wk和Wv是公用的, 乘以xn计算出xn的q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch normalization是计算一个batch的每个句子的第特定个词的z值, layer normalization是计算每个batch的第特定个句子的所有词的z值, 因为每句话的第特定位置的词是很任意的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformer是用注意力机制做的, 注意力机制并没有获得每个词的位置信息, 需要用position embedding在输入层加上位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention是用q,k,v计算出来的, 用Wq, Wk,Wv来乘以x_n来获取第n个输入x的q,k,v, 用q*k/sqrt(dk), 再softmax, 用每个softmax的数值乘以v_n来获得输出. \n",
    "multi-head attention是多个self-attention的组合, 多个self-attention的3个W不一样, 获得的最终的输出也不一样, 最后组合起来, 乘以W获得z值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是一个masked multi-self-attention + layer norm + 全连接层 + layer norm, 这个会叠加多次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把gpt的输出做softmax的映射, 获得分类的概率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第5个词获取v的加权求和时, 只用前4个词的加权值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用WordPiece embedding作为词向量来输入bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用bert的输出做softmax的映射, 获得分类的概率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT是masked multi-self-attention, 只看前面的词, bert看前面和后面, GPT2更深, 参数更多, GPT2比GPT效果更好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
